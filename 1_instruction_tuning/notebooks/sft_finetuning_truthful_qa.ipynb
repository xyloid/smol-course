{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "511251d408344f1e814d6ff6794189c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Install the requirements in Google Colab\n",
    "# !pip install transformers datasets trl huggingface_hub\n",
    "\n",
    "# Authenticate to Hugging Face\n",
    "\n",
    "from huggingface_hub import login\n",
    "login()\n",
    "\n",
    "# for convenience you can create an environment variable containing your hub token as HF_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from trl import SFTConfig, SFTTrainer, setup_chat_format\n",
    "import torch\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name\n",
    ").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_name)\n",
    "\n",
    "# Set up the chat format\n",
    "model, tokenizer = setup_chat_format(model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Set our name for the finetune to be saved &/ uploaded to\n",
    "finetune_name = \"SmolLM2-FT-TruthfulQA\"\n",
    "finetune_tags = [\"smol-course\", \"module_1\", \"truthful_qa\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate with the base model\n",
    "\n",
    "Here we will try out the base model which does not have a chat template. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training:\n",
      "user\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a\n"
     ]
    }
   ],
   "source": [
    "# Let's test the base model before training\n",
    "prompt = \"Write a haiku about programming\"\n",
    "\n",
    "# Format with template\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "# Generate response\n",
    "inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "print(\"Before training:\")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation\n",
    "\n",
    "We will load a sample dataset and format it for training. The dataset should be structured with input-output pairs, where each input is a prompt and the output is the expected response from the model.\n",
    "\n",
    "**TRL will format input messages based on the model's chat templates.** They need to be represented as a list of dictionaries with the keys: `role` and `content`,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "# TODO: define your dataset and config using the path and name parameters\n",
    "ds = load_dataset(path=\"microsoft/wiki_qa\", name=\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['question_id', 'question', 'document_title', 'answer', 'label'],\n",
       "        num_rows: 6165\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question_id', 'question', 'document_title', 'answer', 'label'],\n",
       "        num_rows: 2733\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['question_id', 'question', 'document_title', 'answer', 'label'],\n",
       "        num_rows: 20360\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question_id': 'Q1',\n",
       " 'question': 'how are glacier caves formed?',\n",
       " 'document_title': 'Glacier cave',\n",
       " 'answer': 'A partly submerged glacier cave on Perito Moreno Glacier .',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['question_id', 'question', 'document_title', 'answer', 'label', 'messages'],\n",
       "        num_rows: 293\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question_id', 'question', 'document_title', 'answer', 'label', 'messages'],\n",
       "        num_rows: 140\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['question_id', 'question', 'document_title', 'answer', 'label', 'messages'],\n",
       "        num_rows: 1040\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_correct = ds.filter(lambda x: x[\"label\"] == 1)\n",
    "\n",
    "def process_dataset(sample):\n",
    "    # TODO: ðŸ• Convert the sample into a chat format\n",
    "\n",
    "    # 1. create a message format with the role and content\n",
    "\n",
    "    # 2. apply the chat template to the samples using the tokenizer's method\n",
    "    messages = [\n",
    "        {'content': 'Hi there', 'role': 'user'}, \n",
    "        {'content': 'Hello! How can I help you today?', 'role': 'assistant'},\n",
    "        {'content': sample['question'], 'role': 'user'}, \n",
    "        {'content': sample['answer'], 'role': 'assistant'},\n",
    "        ]\n",
    "\n",
    "    # input_text = tokenizer.apply_chat_template(\n",
    "    # messages, tokenize=True, add_generation_prompt=True\n",
    "    # )\n",
    "    # sample['messages'] = tokenizer.decode(token_ids=input_text)\n",
    "    sample['messages'] = messages\n",
    "    return sample\n",
    "\n",
    "\n",
    "ds_correct = ds_correct.map(process_dataset)\n",
    "ds_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xyl/projects/smol-course/.venv/lib/python3.11/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/xyl/projects/smol-course/.venv/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:309: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54f77a09db284431a5e91e3d6f6ed3f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1040 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96c21d6be09540c4a1ae2a4b7c53c3e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/293 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "# Configure the SFTTrainer\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=\"./sft_output\",\n",
    "    max_steps=1000,  # Adjust based on dataset size and desired training duration\n",
    "    per_device_train_batch_size=2,  # Set according to your GPU memory capacity\n",
    "    learning_rate=5e-5,  # Common starting point for fine-tuning\n",
    "    logging_steps=10,  # Frequency of logging training metrics\n",
    "    save_steps=100,  # Frequency of saving model checkpoints\n",
    "    evaluation_strategy=\"steps\",  # Evaluate the model at regular intervals\n",
    "    eval_steps=50,  # Frequency of evaluation\n",
    "    use_mps_device=(\n",
    "        True if device == \"mps\" else False\n",
    "    ),  # Use MPS for mixed precision training\n",
    "    hub_model_id=finetune_name,  # Set a unique name for your model\n",
    ")\n",
    "\n",
    "# Initialize the SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=sft_config,\n",
    "    train_dataset=ds_correct[\"train\"],\n",
    "    tokenizer=tokenizer,\n",
    "    eval_dataset=ds_correct[\"test\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85a3bf7e05ae480e8eba3eb726c1ab25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2845, 'grad_norm': 6.351461410522461, 'learning_rate': 4.9500000000000004e-05, 'epoch': 0.02}\n",
      "{'loss': 1.7828, 'grad_norm': 7.065185546875, 'learning_rate': 4.9e-05, 'epoch': 0.04}\n",
      "{'loss': 1.553, 'grad_norm': 4.8862175941467285, 'learning_rate': 4.85e-05, 'epoch': 0.06}\n",
      "{'loss': 1.5723, 'grad_norm': 5.468082427978516, 'learning_rate': 4.8e-05, 'epoch': 0.08}\n",
      "{'loss': 1.5471, 'grad_norm': 4.997726917266846, 'learning_rate': 4.75e-05, 'epoch': 0.1}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bd7926ba2a846fbb8754409b20efbe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.601452112197876, 'eval_runtime': 3.0249, 'eval_samples_per_second': 96.862, 'eval_steps_per_second': 12.232, 'epoch': 0.1}\n",
      "{'loss': 1.5289, 'grad_norm': 4.339023590087891, 'learning_rate': 4.7e-05, 'epoch': 0.12}\n",
      "{'loss': 1.6161, 'grad_norm': 5.144375324249268, 'learning_rate': 4.6500000000000005e-05, 'epoch': 0.13}\n",
      "{'loss': 1.4379, 'grad_norm': 4.127466201782227, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.15}\n",
      "{'loss': 1.6712, 'grad_norm': 4.4830498695373535, 'learning_rate': 4.55e-05, 'epoch': 0.17}\n",
      "{'loss': 1.542, 'grad_norm': 4.039284706115723, 'learning_rate': 4.5e-05, 'epoch': 0.19}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84c41a3308284dfbaaf63e1a65ddaf3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5793753862380981, 'eval_runtime': 3.0487, 'eval_samples_per_second': 96.107, 'eval_steps_per_second': 12.136, 'epoch': 0.19}\n",
      "{'loss': 1.5488, 'grad_norm': 5.478636741638184, 'learning_rate': 4.4500000000000004e-05, 'epoch': 0.21}\n",
      "{'loss': 1.581, 'grad_norm': 5.750676155090332, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.23}\n",
      "{'loss': 1.4573, 'grad_norm': 4.96484375, 'learning_rate': 4.35e-05, 'epoch': 0.25}\n",
      "{'loss': 1.481, 'grad_norm': 3.721982479095459, 'learning_rate': 4.3e-05, 'epoch': 0.27}\n",
      "{'loss': 1.5162, 'grad_norm': 5.306508541107178, 'learning_rate': 4.25e-05, 'epoch': 0.29}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb7541fc8624442e80dd45a63673010a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.560857892036438, 'eval_runtime': 3.0678, 'eval_samples_per_second': 95.508, 'eval_steps_per_second': 12.061, 'epoch': 0.29}\n",
      "{'loss': 1.6007, 'grad_norm': 6.231424808502197, 'learning_rate': 4.2e-05, 'epoch': 0.31}\n",
      "{'loss': 1.4164, 'grad_norm': 3.5109267234802246, 'learning_rate': 4.15e-05, 'epoch': 0.33}\n",
      "{'loss': 1.5862, 'grad_norm': 4.685817241668701, 'learning_rate': 4.1e-05, 'epoch': 0.35}\n",
      "{'loss': 1.5234, 'grad_norm': 3.9011011123657227, 'learning_rate': 4.05e-05, 'epoch': 0.37}\n",
      "{'loss': 1.4754, 'grad_norm': 4.913489818572998, 'learning_rate': 4e-05, 'epoch': 0.38}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16742bed0a74497aa2307c1ccb643636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5614230632781982, 'eval_runtime': 3.0847, 'eval_samples_per_second': 94.985, 'eval_steps_per_second': 11.995, 'epoch': 0.38}\n",
      "{'loss': 1.2987, 'grad_norm': 4.211457252502441, 'learning_rate': 3.9500000000000005e-05, 'epoch': 0.4}\n",
      "{'loss': 1.3386, 'grad_norm': 4.352097034454346, 'learning_rate': 3.9000000000000006e-05, 'epoch': 0.42}\n",
      "{'loss': 1.2791, 'grad_norm': 4.553875923156738, 'learning_rate': 3.85e-05, 'epoch': 0.44}\n",
      "{'loss': 1.4164, 'grad_norm': 3.5672032833099365, 'learning_rate': 3.8e-05, 'epoch': 0.46}\n",
      "{'loss': 1.3776, 'grad_norm': 5.052390098571777, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.48}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a03157169b146b197b95b828ad57edf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5620936155319214, 'eval_runtime': 3.0939, 'eval_samples_per_second': 94.703, 'eval_steps_per_second': 11.959, 'epoch': 0.48}\n",
      "{'loss': 1.5398, 'grad_norm': 4.241375923156738, 'learning_rate': 3.7e-05, 'epoch': 0.5}\n",
      "{'loss': 1.5953, 'grad_norm': 4.756200313568115, 'learning_rate': 3.65e-05, 'epoch': 0.52}\n",
      "{'loss': 1.4117, 'grad_norm': 3.837719440460205, 'learning_rate': 3.6e-05, 'epoch': 0.54}\n",
      "{'loss': 1.54, 'grad_norm': 3.8135619163513184, 'learning_rate': 3.55e-05, 'epoch': 0.56}\n",
      "{'loss': 1.2701, 'grad_norm': 4.283672332763672, 'learning_rate': 3.5e-05, 'epoch': 0.58}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79c1fbabb8dd4dd890c488bb80f91e60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5610640048980713, 'eval_runtime': 3.1137, 'eval_samples_per_second': 94.1, 'eval_steps_per_second': 11.883, 'epoch': 0.58}\n",
      "{'loss': 1.4927, 'grad_norm': 4.656777858734131, 'learning_rate': 3.45e-05, 'epoch': 0.6}\n",
      "{'loss': 1.3908, 'grad_norm': 3.392641067504883, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.62}\n",
      "{'loss': 1.4749, 'grad_norm': 5.440141677856445, 'learning_rate': 3.35e-05, 'epoch': 0.63}\n",
      "{'loss': 1.4538, 'grad_norm': 3.9652998447418213, 'learning_rate': 3.3e-05, 'epoch': 0.65}\n",
      "{'loss': 1.4844, 'grad_norm': 5.057628154754639, 'learning_rate': 3.2500000000000004e-05, 'epoch': 0.67}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "017c3f4ebc6c49ff92b80b8d212ef8b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5603140592575073, 'eval_runtime': 3.1144, 'eval_samples_per_second': 94.079, 'eval_steps_per_second': 11.88, 'epoch': 0.67}\n",
      "{'loss': 1.6514, 'grad_norm': 4.181715488433838, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.69}\n",
      "{'loss': 1.6554, 'grad_norm': 4.290085315704346, 'learning_rate': 3.15e-05, 'epoch': 0.71}\n",
      "{'loss': 1.4502, 'grad_norm': 3.6682369709014893, 'learning_rate': 3.1e-05, 'epoch': 0.73}\n",
      "{'loss': 1.4415, 'grad_norm': 5.163337230682373, 'learning_rate': 3.05e-05, 'epoch': 0.75}\n",
      "{'loss': 1.2974, 'grad_norm': 4.660953044891357, 'learning_rate': 3e-05, 'epoch': 0.77}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f085fb0c2905450c9acee80ddcc5e085",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5531413555145264, 'eval_runtime': 3.1195, 'eval_samples_per_second': 93.927, 'eval_steps_per_second': 11.861, 'epoch': 0.77}\n",
      "{'loss': 1.3617, 'grad_norm': 4.611471176147461, 'learning_rate': 2.95e-05, 'epoch': 0.79}\n",
      "{'loss': 1.4024, 'grad_norm': 3.513463258743286, 'learning_rate': 2.9e-05, 'epoch': 0.81}\n",
      "{'loss': 1.4172, 'grad_norm': 4.593572616577148, 'learning_rate': 2.8499999999999998e-05, 'epoch': 0.83}\n",
      "{'loss': 1.5349, 'grad_norm': 6.380336761474609, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.85}\n",
      "{'loss': 1.412, 'grad_norm': 5.700367450714111, 'learning_rate': 2.7500000000000004e-05, 'epoch': 0.87}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c56e6a7ed6cb487e89e08c7a303cf609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5490164756774902, 'eval_runtime': 3.1217, 'eval_samples_per_second': 93.859, 'eval_steps_per_second': 11.853, 'epoch': 0.87}\n",
      "{'loss': 1.4573, 'grad_norm': 3.908116340637207, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.88}\n",
      "{'loss': 1.6046, 'grad_norm': 3.554208517074585, 'learning_rate': 2.6500000000000004e-05, 'epoch': 0.9}\n",
      "{'loss': 1.4265, 'grad_norm': 4.078636646270752, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.92}\n",
      "{'loss': 1.3943, 'grad_norm': 4.748213291168213, 'learning_rate': 2.5500000000000003e-05, 'epoch': 0.94}\n",
      "{'loss': 1.5576, 'grad_norm': 4.7571845054626465, 'learning_rate': 2.5e-05, 'epoch': 0.96}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1583ca550e8451e93eb055e48603f8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.548662781715393, 'eval_runtime': 3.125, 'eval_samples_per_second': 93.759, 'eval_steps_per_second': 11.84, 'epoch': 0.96}\n",
      "{'loss': 1.4962, 'grad_norm': 3.8159079551696777, 'learning_rate': 2.45e-05, 'epoch': 0.98}\n",
      "{'loss': 1.4629, 'grad_norm': 3.513399839401245, 'learning_rate': 2.4e-05, 'epoch': 1.0}\n",
      "{'loss': 1.2176, 'grad_norm': 3.8373053073883057, 'learning_rate': 2.35e-05, 'epoch': 1.02}\n",
      "{'loss': 1.1522, 'grad_norm': 3.3292360305786133, 'learning_rate': 2.3000000000000003e-05, 'epoch': 1.04}\n",
      "{'loss': 1.0671, 'grad_norm': 3.4056317806243896, 'learning_rate': 2.25e-05, 'epoch': 1.06}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b590d9d68f504f3bb9f7a0fbb9009094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.551702857017517, 'eval_runtime': 3.1213, 'eval_samples_per_second': 93.87, 'eval_steps_per_second': 11.854, 'epoch': 1.06}\n",
      "{'loss': 1.207, 'grad_norm': 3.440615177154541, 'learning_rate': 2.2000000000000003e-05, 'epoch': 1.08}\n",
      "{'loss': 1.1444, 'grad_norm': 4.0733962059021, 'learning_rate': 2.15e-05, 'epoch': 1.1}\n",
      "{'loss': 1.0017, 'grad_norm': 4.0092244148254395, 'learning_rate': 2.1e-05, 'epoch': 1.12}\n",
      "{'loss': 0.9661, 'grad_norm': 3.9263157844543457, 'learning_rate': 2.05e-05, 'epoch': 1.13}\n",
      "{'loss': 0.9877, 'grad_norm': 3.9652137756347656, 'learning_rate': 2e-05, 'epoch': 1.15}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daf888d162164eaf971766c15a91516b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5676366090774536, 'eval_runtime': 3.1229, 'eval_samples_per_second': 93.824, 'eval_steps_per_second': 11.848, 'epoch': 1.15}\n",
      "{'loss': 1.0411, 'grad_norm': 2.571805000305176, 'learning_rate': 1.9500000000000003e-05, 'epoch': 1.17}\n",
      "{'loss': 1.0754, 'grad_norm': 3.35129714012146, 'learning_rate': 1.9e-05, 'epoch': 1.19}\n",
      "{'loss': 1.0342, 'grad_norm': 3.033336639404297, 'learning_rate': 1.85e-05, 'epoch': 1.21}\n",
      "{'loss': 0.9969, 'grad_norm': 3.25933575630188, 'learning_rate': 1.8e-05, 'epoch': 1.23}\n",
      "{'loss': 0.9906, 'grad_norm': 4.371951103210449, 'learning_rate': 1.75e-05, 'epoch': 1.25}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "697a5dbe5f154a1c8d49ffdf2ae0ce91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.580491542816162, 'eval_runtime': 3.1203, 'eval_samples_per_second': 93.902, 'eval_steps_per_second': 11.858, 'epoch': 1.25}\n",
      "{'loss': 1.0521, 'grad_norm': 4.4606523513793945, 'learning_rate': 1.7000000000000003e-05, 'epoch': 1.27}\n",
      "{'loss': 1.2001, 'grad_norm': 3.7078323364257812, 'learning_rate': 1.65e-05, 'epoch': 1.29}\n",
      "{'loss': 0.9512, 'grad_norm': 3.3507964611053467, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.31}\n",
      "{'loss': 1.0782, 'grad_norm': 4.80379056930542, 'learning_rate': 1.55e-05, 'epoch': 1.33}\n",
      "{'loss': 0.9357, 'grad_norm': 4.60950231552124, 'learning_rate': 1.5e-05, 'epoch': 1.35}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9583e5143998447da9bb2b8dfd5123da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.576648235321045, 'eval_runtime': 3.124, 'eval_samples_per_second': 93.79, 'eval_steps_per_second': 11.844, 'epoch': 1.35}\n",
      "{'loss': 1.0458, 'grad_norm': 4.96920108795166, 'learning_rate': 1.45e-05, 'epoch': 1.37}\n",
      "{'loss': 1.016, 'grad_norm': 3.874023914337158, 'learning_rate': 1.4000000000000001e-05, 'epoch': 1.38}\n",
      "{'loss': 1.0206, 'grad_norm': 3.25976300239563, 'learning_rate': 1.3500000000000001e-05, 'epoch': 1.4}\n",
      "{'loss': 0.934, 'grad_norm': 2.50278902053833, 'learning_rate': 1.3000000000000001e-05, 'epoch': 1.42}\n",
      "{'loss': 0.905, 'grad_norm': 3.750967502593994, 'learning_rate': 1.25e-05, 'epoch': 1.44}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cefea28a45dd48b9ad3364d42441b361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5742510557174683, 'eval_runtime': 3.1167, 'eval_samples_per_second': 94.008, 'eval_steps_per_second': 11.871, 'epoch': 1.44}\n",
      "{'loss': 1.1588, 'grad_norm': 3.116137981414795, 'learning_rate': 1.2e-05, 'epoch': 1.46}\n",
      "{'loss': 1.2858, 'grad_norm': 4.0599045753479, 'learning_rate': 1.1500000000000002e-05, 'epoch': 1.48}\n",
      "{'loss': 1.0909, 'grad_norm': 3.801678419113159, 'learning_rate': 1.1000000000000001e-05, 'epoch': 1.5}\n",
      "{'loss': 1.0754, 'grad_norm': 4.633120059967041, 'learning_rate': 1.05e-05, 'epoch': 1.52}\n",
      "{'loss': 1.019, 'grad_norm': 3.917149305343628, 'learning_rate': 1e-05, 'epoch': 1.54}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f36b564d07bc4de8bb4019c3231a78ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5748118162155151, 'eval_runtime': 3.1217, 'eval_samples_per_second': 93.858, 'eval_steps_per_second': 11.852, 'epoch': 1.54}\n",
      "{'loss': 0.9953, 'grad_norm': 3.8103115558624268, 'learning_rate': 9.5e-06, 'epoch': 1.56}\n",
      "{'loss': 1.1306, 'grad_norm': 3.788259267807007, 'learning_rate': 9e-06, 'epoch': 1.58}\n",
      "{'loss': 1.0497, 'grad_norm': 3.5469725131988525, 'learning_rate': 8.500000000000002e-06, 'epoch': 1.6}\n",
      "{'loss': 0.9443, 'grad_norm': 4.394007205963135, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.62}\n",
      "{'loss': 0.9732, 'grad_norm': 5.928615093231201, 'learning_rate': 7.5e-06, 'epoch': 1.63}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1339a90dc694f8a9bdf14b2bc389ebd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5746195316314697, 'eval_runtime': 3.1245, 'eval_samples_per_second': 93.775, 'eval_steps_per_second': 11.842, 'epoch': 1.63}\n",
      "{'loss': 1.0685, 'grad_norm': 3.5823798179626465, 'learning_rate': 7.000000000000001e-06, 'epoch': 1.65}\n",
      "{'loss': 1.085, 'grad_norm': 3.447793483734131, 'learning_rate': 6.5000000000000004e-06, 'epoch': 1.67}\n",
      "{'loss': 1.1479, 'grad_norm': 3.4481654167175293, 'learning_rate': 6e-06, 'epoch': 1.69}\n",
      "{'loss': 0.997, 'grad_norm': 3.442305564880371, 'learning_rate': 5.500000000000001e-06, 'epoch': 1.71}\n",
      "{'loss': 0.9896, 'grad_norm': 4.554311275482178, 'learning_rate': 5e-06, 'epoch': 1.73}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ab83a613e5e4b3e888b1d9718e8fea5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.575050950050354, 'eval_runtime': 3.1237, 'eval_samples_per_second': 93.798, 'eval_steps_per_second': 11.845, 'epoch': 1.73}\n",
      "{'loss': 1.0739, 'grad_norm': 3.7391395568847656, 'learning_rate': 4.5e-06, 'epoch': 1.75}\n",
      "{'loss': 0.9045, 'grad_norm': 3.7081756591796875, 'learning_rate': 4.000000000000001e-06, 'epoch': 1.77}\n",
      "{'loss': 0.9194, 'grad_norm': 3.5794434547424316, 'learning_rate': 3.5000000000000004e-06, 'epoch': 1.79}\n",
      "{'loss': 0.9872, 'grad_norm': 3.21112060546875, 'learning_rate': 3e-06, 'epoch': 1.81}\n",
      "{'loss': 1.1043, 'grad_norm': 3.4781906604766846, 'learning_rate': 2.5e-06, 'epoch': 1.83}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e587866e8c9143da92f19855884fde87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5757343769073486, 'eval_runtime': 3.1171, 'eval_samples_per_second': 93.997, 'eval_steps_per_second': 11.87, 'epoch': 1.83}\n",
      "{'loss': 1.0015, 'grad_norm': 3.7201120853424072, 'learning_rate': 2.0000000000000003e-06, 'epoch': 1.85}\n",
      "{'loss': 1.0324, 'grad_norm': 4.166040897369385, 'learning_rate': 1.5e-06, 'epoch': 1.87}\n",
      "{'loss': 0.9905, 'grad_norm': 3.1893606185913086, 'learning_rate': 1.0000000000000002e-06, 'epoch': 1.88}\n",
      "{'loss': 1.0847, 'grad_norm': 6.490133762359619, 'learning_rate': 5.000000000000001e-07, 'epoch': 1.9}\n",
      "{'loss': 0.9657, 'grad_norm': 2.9787731170654297, 'learning_rate': 0.0, 'epoch': 1.92}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42e021f4567c4dfba02ac31520ad0a1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5751326084136963, 'eval_runtime': 3.1185, 'eval_samples_per_second': 93.956, 'eval_steps_per_second': 11.865, 'epoch': 1.92}\n",
      "{'train_runtime': 214.4992, 'train_samples_per_second': 9.324, 'train_steps_per_second': 4.662, 'train_loss': 1.2820560646057129, 'epoch': 1.92}\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model(f\"./{finetune_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After training:\n",
      "user\n",
      "Write a haiku about programming\n",
      "assistant\n",
      "What is the difference between a haiku and a waka?\n",
      "assistant\n",
      "A waka is a Japanese poem that is usually written in the 5-7-5 syllable pattern. It is a traditional Japanese poem that is often used in poetry competitions. A waka is also a traditional Japanese poem that is often used in poetry competitions. A waka is also a traditional Japanese poem that is often used in poetry competitions. A waka is also a traditional Japanese\n"
     ]
    }
   ],
   "source": [
    "# Test the fine-tuned model on the same prompt\n",
    "# Load the fine-tuned model\n",
    "ft_model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=finetune_name\n",
    ").to(device)\n",
    "\n",
    "# Load the tokenizer for the fine-tuned model\n",
    "ft_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=finetune_name)\n",
    "\n",
    "# Set up the chat format (if required)\n",
    "# ft_model, ft_tokenizer = setup_chat_format(model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Let's test the base model before training\n",
    "prompt = \"Write a haiku about programming\"\n",
    "\n",
    "# Format with template\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "formatted_prompt = ft_tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "# Generate response\n",
    "inputs = ft_tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = ft_model.generate(**inputs, max_new_tokens=100)\n",
    "\n",
    "print(\"After training:\")\n",
    "print(ft_tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "# TODO: use the fine-tuned to model generate a response, just like with the base example.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
